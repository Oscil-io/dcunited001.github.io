---
title: "Divergents and Directional Antennae - Measurement and Inference"
categories: blog
tags: "math science physics"
headline: "How many experimental measurements?"
author:
  name: "David Conner"
excerpt: "
"
---

### How Many Experimental Measurements?

Update... One More Question, Actually

How many experimental measurements does one need to make in order to
infer the distribution of energy?  Is it less or more?  For a
quadratic equation, you need 3 measurements. For a cubic equation, you
need four.  And generally, for any n-polynomial in two dimensions, you
need n+1 measurements.

But for determining this kind of distribution, how many measurements
do you need to make to be more confident in the rest of the values?
When determining the average value for EMR in any `SVE` over a short
time-scale? And when you take into account how the values for each
`SVE` and `AVE` fluctuate over time?

Is there the same kind of relationship as with n-polynomial equations
in two dimensions? Where more information allows you to confidently
make more inferences?  Or is there some maximum confidence with which
you can infer the distribution because of the nature of the mathematic
model? Can you fluctuate the power of the antenna when you make
measurements and use the derivative of EMR value with respect to the
derivative of the power fluctuation to make better inferences?  I'm
pretty sure you can.

### Noise Confers Intuition

On the surface, it would appear that you have to make more
measurements the more complicated the relationship is, but I contest
that the more complicated the relationship, the more specific it's
influence -- and therefore, the *easier* it is *sometimes* to make
more confident measurements.

And especially, if you can inject some known pattern of noise into the
system, you can generally make inferences that allow you to more
confidently understand the nature of the system, even though "it's
just random noise." This is especially true if the derivative is
useful because you can look for the signal of the derivative of the
noise transformed in the output.

### When Methodology Affects Data

When the methodology affects the data you collect, this makes thex
inference of something like the distribution of EMR so much more
difficult. This is because you can never truly measure the thing you
want because the instrument or whatever introduces noise that changes
the values.  So you're measuring a system that's completely different
from the one you want to understand.  It basically turns your systems
into something like differential equations.

However, the analogy to linear polynomial equations in two dimensions
above applies here: you're just comparing systems of equations, not
values for points.  So, to understand the "zero system" which is like
the system of equations which you haven't actually affected with your
methodology for measurement, you just have to observe enough kinds of
"non-zero systems." Then you should be able to cancel things out and
infer the nature of the "zero system" you're looking for.

### Statistical Resolution

When you add the "dimension" of statistics to your mathematics models,
you are empowering yourself to understanding the likelihood of a
system to assuming various states.  This allows you to solve the
problems from a completely different perspective.  You almost don't
need to know anything about how the system works to begin reducing it
further.  You're basically just eliminating space until you find an
analytic system that at least simulates the numeric system you're
looking for.

This is similar to how the LHC engineers looked for the Higgs Boson.
They kept making measurements with experiments at high enough energies
until they reached sixth sigma.  They almost certainly eliminated the
possibility of a particle with energies of the Higgs *not existing*.

### Artificial Intelligence

But you can pretty much apply this statistical dimension to any system
and whittle away at the space of things that are not true, leaving you
with a better understanding of what must be true.  This scares the
shit out of me, in the context of AI.  It basically means that the
more information and data that AI has to process, the more completely
it can understand a system and the more efficiently it can do so.

This can also delude people, who, when given a deluge of information
about a system, can be led to making the wrong decisions about it by
hyperfocusing on one subsystem within it.  E.G. when people have
security cameras, they feel safer because they think they see
everything going on, but often their eyes lie to them.  But this
doesn't really work with AI because machine learning algorithms can
process information, the significance of which it doesn't need
conscious awareness.  Yes, bias is a huge problem in machine learning,
but given enough processing power and enough of the right kind of
data, it will always produce frighteningly accuracte inferences about
systems.

### Noise Injection X Statistical Imaging

This, as I have noted on twitter, is very useful in understanding the
results of MRI, especially diffusion-tensor imaging.  We should be
able to conbine data from various methodologies.  It's all lined up
with the same physical space as a domain generating the MRI data.
Though the individual spaces of each MRI methodology may be different,
they can all be correlated to the same 3D cartesion physical space,
relative to the person being imaged.
