---
title: "Divergents and Directional Antennae - Measurement and Inference"
categories: blog
tags: "math science physics"
headline: "How many experimental measurements?"
author:
  name: "David Conner"
excerpt: "
"
---

### How Many Experimental Measurements?

Update... One More Question, Actually

How many experimental measurements does one need to make in order to
infer the distribution of energy?  Is it less or more?  For a
quadratic equation, you need 3 measurements. For a cubic equation, you
need four.  And generally, for any n-polynomial in two dimensions, you
need n+1 measurements.

But for determining this kind of distribution, how many measurements
do you need to make to be more confident in the rest of the values?
When determining the average value for EMR in any `SVE` over a short
time-scale? And when you take into account how the values for each
`SVE` and `AVE` fluctuate over time?

Is there the same kind of relationship as with n-polynomial equations
in two dimensions? Where more information allows you to confidently
make more inferences?  Or is there some maximum confidence with which
you can infer the distribution because of the nature of the mathematic
model? Can you fluctuate the power of the antenna when you make
measurements and use the derivative of EMR value with respect to the
derivative of the power fluctuation to make better inferences?  I'm
pretty sure you can.

### Noise Confers Intuition

On the surface, it would appear that you have to make more
measurements the more complicated the relationship is, but I contest
that the more complicated the relationship, the more specific it's
influence -- and therefore, the *easier* it is *sometimes* to make
more confident measurements.

And especially, if you can inject some known pattern of noise into the
system, you can generally make inferences that allow you to more
confidently understand the nature of the system, even though "it's
just random noise." This is especially true if the derivative is
useful because you can look for the signal of the derivative of the
noise transformed in the output.

### When Methodology Affects Data

When the methodology affects the data you collect, this makes thex
inference of something like the distribution of EMR so much more
difficult. This is because you can never truly measure the thing you
want because the instrument or whatever introduces noise that changes
the values.  So you're measuring a system that's completely different
from the one you want to understand.  It basically turns your systems
into something like differential equations.

However, the analogy to linear polynomial equations in two dimensions
above applies here: you're just comparing systems of equations, not
values for points.  So, to understand the "zero system" which is like
the system of equations which you haven't actually affected with your
methodology for measurement, you just have to observe enough kinds of
"non-zero systems." Then you should be able to cancel things out and
infer the nature of the "zero system" you're looking for.

### Statistical Resolution

When you add the "dimension" of statistics to your mathematics models,
you are empowering yourself to understanding the likelihood of a
system to assuming various states.  This allows you to solve the
problems from a completely different perspective.  You almost don't
need to know anything about how the system works to begin reducing it
further.  You're basically just eliminating space until you find an
analytic system that at least simulates the numeric system you're
looking for.

This is similar to how the LHC engineers looked for the Higgs Boson.
They kept making measurements with experiments at high enough energies
until they reached sixth sigma.  They almost certainly eliminated the
possibility of a particle with energies of the Higgs *not existing*.

### Artificial Intelligence

But you can pretty much apply this statistical dimension to any system
and whittle away at the space of things that are not true, leaving you
with a better understanding of what must be true.  This scares the
shit out of me, in the context of AI.  It basically means that the
more information and data that AI has to process, the more completely
it can understand a system and the more efficiently it can do so.

This can also delude people, who, when given a deluge of information
about a system, can be led to making the wrong decisions about it by
hyperfocusing on one subsystem within it.  E.G. when people have
security cameras, they feel safer because they think they see
everything going on, but often their eyes lie to them.  But this
doesn't really work with AI because machine learning algorithms can
process information, the significance of which it doesn't need
conscious awareness.  Yes, bias is a huge problem in machine learning,
but given enough processing power and enough of the right kind of
data, it will always produce frighteningly accuracte inferences about
systems.

## Noise Injection X Statistical Imaging

This noise injection, as I have noted on twitter, is very useful in
understanding the results of MRI, especially diffusion-tensor imaging.
We should be able to conbine data from various methodologies.

### Spatial Coherence

First, to link images spatially; to get spatial coherence between
image sets.  There are several types of MRI -- T1 and T2 is one
distinction.  Whether or not a dye was used is another dimension.
There are sagital, axial, and coronal image sets.  It's all lined up
with the same physical space as a domain generating the MRI data.
Though the individual spaces of each MRI methodology may be different,
they can all be correlated to the same 3D cartesion physical space,
relative to the person being imaged. This means that specific
structures in the brain, varying in how the data specifically depicts
them, can be correllated across images.

This technique can link multiple types of MRI images, so they can be
corrected into the same spatial coordiante system and their values can
be combined.  Their values would be spatially shifted though, so
standard algorithms like binomial image interpolation would not work
without some customization.  However, the higher resolution the MRI,
the less effect that spatial dissonance would have on image analysis,
since data from the images would be more spatially "in phase."

### Temporal Coherence

The next step is to get temporal coherence.

- short term
- long term

### Categorical Coherence


One final step is to get categorical coherence, which requires
understanding more about the input in order to make inferences across
data sets.  For MRI, this means understanding the patient's age,
recent/chronic diet/nutrition habits, medical status, etc.  So that
inferences can be made across data sets.  I.E. without spatial or
temporal coherence, the hippocampus could be recognized in a axial
image by an algorithm trained to identify hippocampus shapes at
various ages, young or old.

This "categorical coherence" looks for higher level patterns.  It
could identify the presence of diseases previously unknown, not
because it's matching against a list of diseases and their
progressions, but because it's identified unknown patterns of
neurodegeneration in a patient.  In addition to unknown disease
progression, this could also identify the combination of drug-induced
neurological damage or damage induced by the combination of diseases.
It could even identify WHAT DRUG CAUSED THE NEUROLOGICAL DAMAGE by
understanding the dispersion and extend of damage in the brain.

### Drug absorption in the Human Brain.

The brain is very particular in that it has the blood brain barrier,
which restricts entry of charged particles.  The only way a charged
particle can pass through the blood brain barrier, which is a linining
of brain cells that receives and filters nutrients from arteries and
capallaries, is if that charged particle is briefly neutralized.  The
point is that this has a big effect on the dispersion of specific
chemicals throughout the brain, which form the basis of a "neural
fingerprint" for neurological damage or for normal activity and
absorption.


it doesn't seem to matter how smart or talented you are
- there are so many intelligent peeople out there
  - that it's easy for many people to get filtered out if they don't
    conform
  - especially since for any piece of knowledge to be replicated, it
    only has to be incontrovertibly demonstrated once.
  - after that, the person who reached those conclusions is no longer
    necessary. perhaps, even less necessary than before, in some
    circumstances.
  - that attitude, i don't believe that's right.


sufficiently intelligent being should evolve into one of several
*types*, given the proper experiences
- but a being that is more intelligent than that should evolve into
  one that can embody all of them.
