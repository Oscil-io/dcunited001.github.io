---
title: "Social Physics: Misc Ideas"
categories: "blog"
tags: "oneirontology religion semiotics linguistics"
headline: ""
author:
  name: "David Conner"
excerpt: ""
---

when i look at this writing without being on an amphetamine, it feels
like I'm reading something a fucking alien wrote.

# Blog Ideas:

## The Basics & Why It's Personally Relevant

- particle physics crossed with graph theory
  - with biophysics-like state-machines, with idealized particles and
    less-complex physical state
- social physics is like a (macrocosm) of biophysics

## A Typology of Occupations

#### A typology of jobs from a social physics perspective

- how to choose an occupation based on maximizing social physical
  preferences:
  - targeted demographic,
  - varied demographics,
  - more control over dynamically targetable demographic,
  - high flux of particles/people
  - highly varied flux of particles/people

#### Objectives Someone Might Want to Achieve Via Their Occupation

- limit total social exposure over time
- develop their own social graph
- encounter local graph flux

#### Example Jobs to Delineate

- MC/DJ
- Event Coordinator
- Wedding Planner
- Prison Guard
- Barkeeper
- Cop
- Talk Show Host
- Cook
- Pastor
- Narc
- Office (customer facing and not)
- Production Engineer

## Power Can Be Defined as the Potential to Trigger Flux through the Social Network

- this flux comes in many forms, including the induction of:
  - physical change
  - informational change
  - schematic change
  - behavioral change

- the forms above are higher-order. that is, what we see IRL are
  really combinations of the above:
  - informational change that results in physical change
  - informational change that results in eventual schematic change on
    a social scale
  - behavior change that results in long-term physical changes
  - physical changes that result in informational changes
  - schematic change that results in self-perpetuating changes over
    the course of years, decades or centuries (i.e. religion)

- this potential for flux/action is very much dependent on the beliefs
  of nodes in the system and a differential of information available
  to the nodes.
  - that is, if there is already complete disemination of some pieces
    of information to a set of nodes, you have reduced or lost the
    capacity to influence those nodes through the disemination of that
    information
    - this is overly simplified because, again, everything is a
      lower-order conjugation of the above higher-order forms
    - differential equations
  - but the social graph that emerges on top of physically interacting
    particles (idealized people) in a social system functions very
    much like an electrical network
    - in this way, power and information function similarly to
      voltage, amperage, gates, etc, albeit in an extremely-high
      dimensional way
    - math problems that are extremely high in dimension or infinitely
      dimensional function in radically different ways
      - this often studied as the curse of dimensionality and it's
        implications in various areas of math diverge widely

- the inability to scalably model or to even ascertain pieces of
  information, which are sometimes unindexable if no rigid schema can
  be devised, means that the way in which information is handled in
  these models becomes infinitely more important
  - i.e. convergence and homogeneity of belief systems is required to
    make *any* useful inferences and those aspects of the systems
    which cause schematic divergence become problematic

## Single Most Important Conclusion from Learning Social Physics

- the single most important conclusion I can draw from a mathematical
  assessment of social physics is that the ability to make valid
  assumptions on information/beliefs and the behaviors of nodes is
  unparalleled
  - if you cannot do these things, you cannot expect to predict
    specific or general behavior and you cannot expect to extrapolate
- this leads into a discussion of schematic complexity

- a second critical factor is the lack of multitasking:
  - particles can only really do one thing & can only think one thing
    at a time, which bounds how quickly state changes can occur ##
    Discussion of Schematic Complexity

- complexity of information crossed with type theory
- social physics simulations are rooted in particle physics w/ shared
  fields -
  - the interactions in the physical space are dependent on n-D
    primary virtual space(s)
    - as well as combinatorially enumerated secondary virtual spaces
      which emerge
  - these virtual spaces can be parameterized with continuous or
    discrete variables
    - from which all the classic types of topological spaces emerge
- on the other end, the physical spaces (the "real" spaces) can
  provide for various particle behaviors and behavior types -
- this is the most complex intersection of infinities in every way I
  can conceive -
- it's the most complext application of applied pure math that I have
  yet imagined
  - but social physics is all mostly based on work done in the 19th
    century
- and yet, we all navigate this mathematically intractible maze with
  the same 3-pound slab of smart-meat
  - some of us are profoundly terrible at it.
  - a few operate methodically, including both sociopaths and empaths,
    balancing opportunity cost to empower themselves or others
  - Most of us meerkats are as adept as our range and sum of
    experience. This category operates socially without significant
    conscious effort, which implies several:
    - (1) They have one to five prior years of constant/regular social
      experience with few total or involuntary interruptions,
      regardless of whether it's positive or negative
    - (2) They have a wide variety of types of experiences
    - (3) They have a strong degree of choice in social experience,
      which requires and produces social independence
    - (2) They haven't been unhappy in their lives long enough to wonder
      why they can't get it together.
    - (3) When they have the kinds of problems almost everyone does,
      they have enough of the right social capital or psychological
      flexibility to adjust before they become socially isolated or
      financially disabled.



- most of us lack a rigid framework: we subconsciously operate
  emotionally-driven and neurally-connected statistical models
    - basically cross graph theory, markov models & state machines
    - behavior and state-changes driven by:
      - (1) rules whose rigidity ranges from diamond to steel to paper
      or plastic.
      - (2) information, confidence of information, distribution of
        information
      - (3) individual and collective objectives, needs, wants,
        resources and options

- social isolation can be measured, given some "normal" facebook
  user's 2-degree graph of friends for some definition of normal
  (casual but regular) social media usage
  - filter the friends by strength of connection to indentify edges
    sending and receiving inbound and outbound messages/interactions
    - measure the degree of reciprication and the variety of people
      forming the strongest recent connections
    - filter out all connections which do not fit some minimum of
      interaction threshold
    - this sparsifies the graph, filtering a dense graph and leaving
      only the most relevant connections
  - from here, the person's isolation can be measured using spectral
    graph theory
    - this independence can be measured in graph theory with
      kelman's transformation
    - see spectral graph theory
      [here](http://www.cs.elte.hu/~csiki/application_Kelmans.pdf),
      [here](http://www.math.ucsd.edu/~mtait/k-independent.pdf)
      - and [here](https://www.cs.elte.hu/math/phd_th/csiki.v2.pdf)
      - the details of this math is over my head btw

- this, of course, assumes the person is a "normal" facebook user,
  where the relevant 1-degree and 2-degree connections in the friend
  graph are all also "normal" users
  - this high-level algorithm could also work with clustered types of
    facebook users
    - yet, the degree in variation in how people use applications to
      communicate and their distribution of time/usage across multiple
      applications makes clustering extremely difficult.
    - at least, if a clustering algorithm is to enable the selection
      of users which fit some basis with quantifyable variation and
      some metric of confidence

# A System With Near-Total Lack of Privacy

- (none of this means technology or social media or IoT is inherently
  bad by the way, just that our world is evolving towards a
  fundamentally different one and this evolution is inevitable)
  - so we have to understand it because the following philosophical,
    psychological, and sociocultural changes are fundamentally
    different in a way that is very difficult to understand
  - the most important: in a post-convergence world, causality itself
    operates in a fundamentally different way, which can be
    anticipated by observing the evolution of various graphs

- social media, data science, internet of things, etc are part of a
  continuing trend of technology and quantification of human behavior
  that has been eroding privacy for decades.
  - this causes a fundamental shift in the way people interact

  - information can now spread through the social graph in
    fundamentally different ways.
    - the social graph has always existed, but until the 19th century
      was almost entirely limited to communication via edges which
      were physical in some way)
    - instead of a world where every person has six degrees of
      separation, we are gradually converging towards a social
      paradigm where "degrees of separation" is mostly meaningless, in
      and of itself.
      - it's because the graph that represents all the paths that
        information can take to reach some other human node is
        converging towards the complete graph
        - albeit a complete graph where nearly all edges would be
          removed upon sparsification, but the phenomenon of
          near-completeness is there
    - at the same time, we are producing and processing data at an
      exponential rate.
      - there are finitely many human behaviors that can be quantified

- there is a paradigm shift in the phenomenon of causality, as it
    applies to human behavior and psychosocial dynamics,
    - causality, in this sense, stems from exchange of information,
      completeness of information, awareness of information,
      dissemination of information, etc.
      - there are many other important factors that help define how
        humans perceive causation in their behavioral interactions,
        whether those are individuals or groups of people ....
      - however, causality and the way humans utilize their
        understanding of it to construct a of beliefs/knowledge about
        the world is in a very big way parameterized by the paradigm
        of information exchange/retention/diffusion/etc
    - therefore, in an increasingly connected world, there is a major
      shift in how narratives are constructed and propagated.
  - this paradigm shift for causality affects any information
    processing entity, including both people and computers
    - this shift affects these groups in different ways
    - people store & process & propagate information
      differently than computers
      - i.e. they tend to be arbitrary and "imperfect" in how they
        handle information
      - the conclusions that can be reached by understanding the shift
        in causality very greatly depending on the specifics, so
        assertions based on generalities may do more harm than good
        (e.g. this explanation itself of the shift in causality is
        overly general)

- the concept of privacy is one of the most important ways this shift
  in causality will affect human interactions and their understanding
  of the world from what they infer to be cause & effect in events
  - individuality will be blurred, people may adopt more
    groupthink-motivated behaviors. these two things are fairly simple
    to understand as our privacy is eroded
  - if or when we reach a society with a near-total lack of privacy,
    information propagates with a degree of completeness that implies
    people are significantly impaired in their ability to reason about
    the nature of cause and effect
    - this is because every exchange in units of information
      essentially initiates its own new chain of cause/effect and
      because of the total-convergence of the systems used to exchange
      information/data, ...
    - for example, making conclusions about the disemination of pieces of
      information (how one piece of information spreads amoungst
      people)

- so the convergence of individual belief systems accelerates and the
  more strongly convergent metaphyiscal ideas begin manifesting in the
  physical aspects of social interactions.
  - so, if instead of mass communication channels being completely
    decentralized, the information diseminated was harmonized, then
    the more unitary, regular and pronounced these metaphysical
    manisfestations will become.
  - so, in terms of social physics, there are stronger physical
    effects that emerge from the metaphysical than ever before. This
    is true even in the discord of decentralized high bandwidth
    digital communication. Because of the evolutionary dynamics of
    ideas and memes, the strongest earn extremely high rates of
    propagation.

### Distribution of Agency & Will (Animus)

- the dynamics of physical/informational interactions of agents/nodse
  in graphs: social/physical/financial/etc
  - lots more to explore here

### Social Media

- manipulation of social media algorithms to exclude "undesirable"
  individuals is akin to segregation. in a convergent social network
  that is increasingly less engaged in "real" interactions and more
  engaged in "virtual" interactions, where some interactions can be
  voluntarily or involuntarily censored via SEO/Social Media
  manipulation, this means that entire groups of people can become
  silenced or disenfranchised without even knowing it.

# Dr Strange

### Power and Mitigating Counter-Ancipitory Effects in One's Environment

- Dr Strange has a high focus on and a high density of
  counter-anticipitory effects
  - this is true, visually, semantically and in-terms
  - it's almost a story where, in terms of plot, time itself is
    rearranging itself, even though the movie is perceived linearly

> Beyond Time, Beyond Death...

where there are aspects of the metaphysical that transcend time and a
living agent's capacity to project change into the metaphysical beyond
their own subjective demise is perhaps the greatest power.

#### "Unintended extrapolations are always the most severe"


#### "A Phase Change in the Functional Fabric of Reality"

- nature of duality vs agency, along geographic/philosophic dimensions
- an agent's degree of influence and perceived influence over
  environment, especially as it relates to some permanent state
  changes.
  - in particle systems which are high-dimensional and where the
    particles have agency and intentionality, phase changes among
    particles have a far greater capacity for effecting other
    particles in the system.

- for the common person to understand, they need to understand what a
  phase change in physics actually is and, most importantly, what a
  phase change is defined as outside of physical systems.
  - if you think of solid, liquid, gas, plasma, BEC ... what makes
    each one what it is?
    - they are composed of the same particles, mostly.
    - why is it that they "seem" so different, qualitatively?
    - how do you define what conditions lead some substance to begin
      to transform itself into a different phase?

- how do you model the interactions between a mixture of a single
  substance where different parts of the space you're observing are in
  different phases?
    - this is the most important part of understanding what a phase
      change is and carrying it over to non-physical systems: how does
      a physicist mathematically model different phases of a
      single-substance system and *why*?
  - this is what distinguishes systems with math that can model phases
    and those that can't.
    - phase changes introduce and enable dramatically different modes
      of interaction between the substance and both it's consitutuent
      parts and it's environment.
    - for example, solids propagate energy as more clearly defined
      waves moving across a more limited set of crystal configurations
      - liquids and gases also propagate waves of energy across the
        system, but there are more physical degrees of freedom for how
        a particle can kinetically react.
      - but, this qualitative change in how a particle reacts to its
        envrionment emerges not from signal particle or the state of
        its local environment, but from more macroscopic relationships
        that emerge from a *group* of particles.
        - that is, it's *difficult* to say whether a single atom of
          carbon is a particular phase at any temperature.
          - the carbon atom just acts like a carbon atom and nothing
            else.
        - as a aside: this is also why various configurations of
          matter at the nano scale can have such dramatically varied
          behaviors that do not appear on more macro scales and do not
          appear for singular atoms and molecules
        - some of these properties are chemical and electrical and,
          thus, are not traditionally considered "phases" by chemists
          or physicists.
          - physical phases are defined as being completely separate
            from the chemical properties of the atom or substance.
        - but the "phase" of a region of a substance determines how
          that section of matter interacts with matter of the same
          substance in other adjacent slices of space.
          - that is, even though it's very literally the same
            substance, the "rules" for how it physically interacts
            change.
- and so, there are phase changes in many kinds of systems. they are
  not just confined to physical systems. some chemical properties of
  matter can be dependent on the substance's phase.
  - some chemical properties that only emerge on the nanoscale could
    be modeled as "phases" although they are not related to the
    physical phases of solid, liquid or gas.
    - and so, as you zoom out to larger collections of the same
      substance, it loses those properties from the nanoscale
      configuration of matter, unless that fragile nanostructure is
      held consistent across the entire material.

- soooo ... what are phase changes in sociophysical systems?


### Configurational Entropy in Sociophysical Systems

- need to review the concepts here:
  - conformation space are the probabilities of seeing particular
    conformations of molecules, which AFAIK is not concerned with the
    orientation of molecules (at least not the probability
    distribution of conformations)
  - configuration space has more to do with valid and realistic
    arrangements of molecule types throughout a system.
    - 


prevalence of )
- as a sociophysical system progresses, you can keep track of the
  amount of time specific "molecular" configurations of participants
  spent connected.
  - this forms a kind of combinatorially-derived entropy, but it's not
    thermodynamic entropy. it's closer to Shannon's informational
    entropy.
  - you can also keep track of the *types* of configurations that
    emerge. this is probably more important, considering there are
    factorially more combinations with more particles.
    - if there are multiple particle types, then each counts as a
      symbol for combination. (i.e. AA/AB/BA/BB for types A & B)
  - this is useful because you get feedback on understanding how the
    rules of your simulation affect the kinds of interactions that
    emerge amoung particles.
    - and it can help you identify when the rules of the system are
      weighted improperly, leading to lower configuration entropy and
      a skewed distribution

- this concept carries over to organic chemistry, where it's called
  configuration entropy. there is also conformational entropy, but
  this is a weaker analogy b/w organic chemistry reactions and social
  physics simulations.
  - you could say that a social physics simulation with arbitrary
    rules yields conformations of nodes, some more or less stable
  - but, the types of conformations for a group of molecules are less
    defined. it's probably not as useful without some means of
    restricting the types of conformations that emerge by placing
    constraints on particle interactions


# Social Physics Simulation Ideas

[for a 2-force simulation, where one force operates with orientation](https://peeterjoot.wordpress.com/2009/12/08/jacobians-and-spherical-polar-gradient/)

#### cocktail party simulation

  - 100-1000 particles form molecule "conversations"
  - two fields and simple particle physics determine particle movement
  - no exchange of information

#### bar simulation

  - build on cocktail party simulation
  - particles are of a single type
  - variations: add money, add gender

#### colorshifting FFT (not really social physics)

  - each particle has a colorshifting function, which takes its
    current state and some parameters, then transforms the color
  - each FFT segment amplitude determines the mass/speed or field
    reactivity of the particle
  - so red particles move fast, blue particles move slow and FFT
    partially determines the amount of movement for each particle
    - as well as field interactions

#### marijuana dealing simulation

  -


#### Viral Nature of Language

- 100 simple particles, each with a dictionary of 100 english nouns
  mapped to integers

  - each time a connection is forged, they exchange:
    - their copy of that dictionary
  - connected via graph constructed by delauney triangulation


#### Cymatic Particle Shader

- model the distribution of waves through a 2D lattice of connected
  particles.
  - stream audio into some points and model waves as they propagate
    through the lattice, without losing information
  - model the wave transmission in a similar way to conway's game of
    life.

### Measuring Configurational & Confirmational Entropy in Social Physics

- instead of measuring Thermal Entropy via "Course Graining" of q[i] &
  p[i] (according to wikipedia)
  - and dilineating the eexpected/actual distro's with the ideal...

- instead, these are simulations to study the sociophysical
  "molecular" distributions found in configuration space after a
  simulation runs.
  - in particular, identify the total time summed for each molecule
    type ... and hopefully specific combinations of particles by ID,
    but those calculations will be mostly intractable after a few
    dozen particles
  - how do these typologies of connected social particles become more
    and less prevalent over time?
    - I'm not sure if this question can be answered. it is possible,
      if you're simply measuring the types of particles that
      emerge. not particle ID combinations.
    - even still, it will require storing large aggregate textures
      over time

- this requires the standard social physical particle system
  determined by at least 2 forces upon which higher order, graph
  mediated connections accrue
  - the graph is the product of particle positions and is formed when
    node's attractive forces overlap, stabalizing against the
    repulsive forces. this is similar to electronegativity and the
    strong positive charge at the center of molecules
    - the attractive force is better understood as a field of vision
      and it's not the field that directly affects particles, but the
      area of space which directs a higher density of particle
      "attention"
      - this itself forms a field, which triggers higher-level actions
        in the particles. this field of attention is a secondary field
        over which the force can transfer energy
      - (move this summary up to the initial description of this type
        of simulation)

  - attractive field maxima are identified and these centroids are
    estimated to be the center of molecules.
    - the centroids can be pulled out with a cheap knockoff of
      thrust's scan() method.
    - these centroids are through into memory as vertices for delauney
      triangulation, along with the rest of the particles.
    - from here, a graph can be constructed in matrix form
      - if the indices of the maxima are retained, you can walk
        outward one degree from the centroids with the CPU
        - (under assumptions in the physical behavior of nodes,
          e.g. stability)

- the result is that you get subgraphs with a very finite set of
  possible molecule, 2 through 7 particles at most arranged in circles
  - you can use these subgraphs to add the frametime of each
    configuration-type to it's bin in the accumulating texture
    - the molecules are only speciated into types by their subgraph
      edges in a molecule, not by their orientation in space.
    - for particle simulations with a single particle type, only 6 or
      7 molecule types will emerge.
      - for simulations with multiple particle types, there are more
        types that can be formed.
  - the values in the accumulating texture are related to the
    probability distribution for seeing these molecules in the social
    physics simulation, or may actually be

- from here, entropy can be calculated for the distribution of
  molecule arrangements in time.
  - all of the following will change the distribution for types of
    molecules that emerge:
    - different particle types and rules for their interactions
    - particle interactions are defined by particle state & state
      changes
  - given that you know the inherent variations in particle attributes
    that contribute towards their action/interactions, there should be
    some "normalness" in the distribution of combinations/subgraphs &
    their qualities

- amoung simulations with a lower particle to particle-type ratio,
  properly defined rules and balanced values should result in higher
  entropy values.
  - balancing these equaations and forces seems difficult. the best
    way to do it is a combination of a numerical approach and an
    automated approach.
  - numerically, you can identify values of rForce & aForce that
    result in a net force of zero that hold particles in place
    - this must be done carefully and so that it works for all
      two-to-seven particle molecule AND for all clusterings of
      particles that don't quite meet the definition of a molecule
    - this utilizes the lagrangian, i think
    - numerically tuned values can give you a consistent basis each
      amoung the rForce & aForce, which will be important.
    - the basis has to be set up to scale properly to different screen
      resolutions, which is something that i messed up recently


